{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "colab_type": "code",
    "id": "SkdCdw9FG3ix",
    "outputId": "2cafb6f2-7852-41f3-b963-35b4cb9a6f20"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-516461c3d458>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2 \n",
    "import numpy as np \n",
    "import math \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7K70NaXPF71P"
   },
   "source": [
    "### Define number of train and test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJIFnGULyjNs"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nr3_UbcxGD6U"
   },
   "source": [
    "## Generate and save Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "X6FTLkMczMr5",
    "outputId": "c6ed8838-870d-42c9-b138-e72358ad967d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['right', 'left', 'none']\n",
      "class:  right\n",
      "class:  left\n",
      "class:  none\n",
      "704\n",
      "(704, 32, 32, 3) (704, 128, 128, 3) (704,)\n"
     ]
    }
   ],
   "source": [
    "path = '/content/drive/My Drive/Drive2(Panasonic)/Dataset/CelebA/'\n",
    "classes = os.listdir(path)\n",
    "train_size = 704\n",
    "file_size = 128\n",
    "file_size2 = 32\n",
    "print(classes)\n",
    "index = 0\n",
    "counter = 0\n",
    "X_train = np.zeros((train_size, int(file_size2), int(file_size2), 3), dtype = 'uint8')\n",
    "Y_image = np.zeros((train_size, int(file_size), int(file_size), 3), dtype = 'uint8')\n",
    "#Y_image = original image\n",
    "#X_train = downscaled image\n",
    "Y_train = np.zeros((train_size), dtype = 'uint8')\n",
    "\n",
    "for i in classes:\n",
    "\n",
    "    print('class: ', i)\n",
    "    files = os.listdir(str(path) + str(i))\n",
    "\n",
    "    for k in files:\n",
    "        img = Image.open(str(path) + str(i) + '/' + str(k))\n",
    "        img.load\n",
    "        img = img.resize((int(file_size), int(file_size)), Image.ANTIALIAS)\n",
    "        img_down = img.resize((int(file_size2), int(file_size2)), Image.ANTIALIAS)\n",
    "        npimg = np.asarray( img, dtype=\"uint8\" )\n",
    "        npimg_down = np.asarray( img_down, dtype=\"uint8\" )\n",
    "        X_train[counter,:,:,:] = npimg_down\n",
    "        Y_image[counter,:,:,:] = npimg\n",
    "        Y_train[counter] = classes.index(i)\n",
    "        counter += 1\n",
    "    \n",
    "      \n",
    "print(counter)\n",
    "\n",
    "np.save('/content/drive/My Drive/Drive1(Own)/Arrow224/X_train.npy', X_train)\n",
    "np.save('/content/drive/My Drive/Drive1(Own)/Arrow224/Y_train.npy', Y_train)\n",
    "np.save('/content/drive/My Drive/Drive1(Own)/Arrow224/Y_image.npy', Y_image)\n",
    "print(X_train.shape, Y_image.shape, Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ea7SrDGdGrne"
   },
   "source": [
    "## Load and Normalize Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SZgSCElKIwOl",
    "outputId": "a08b6225-6152-43e4-83cd-fb2a76c30fcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 32, 32, 3) (20000, 256, 256, 3) (20000, 1)\n"
     ]
    }
   ],
   "source": [
    "path_load = '/media/dl/DL/aashish/upscaling/sr_dataset/'\n",
    "X_train = np.load(path_load + 'X_train.npy')\n",
    "Y_image = np.load(path_load + 'Y_image.npy')\n",
    "Y_train = np.load(path_load + 'Y_train.npy')\n",
    "num_classes = 1\n",
    "'''X_train = np.load('/content/drive/My Drive/Drive1(Own)/Arrow224/X_train.npy')\n",
    "Y_image = np.load('/content/drive/My Drive/Drive1(Own)/Arrow224/Y_image.npy')\n",
    "Y_train = np.load('/content/drive/My Drive/Drive1(Own)/Arrow224/Y_train.npy')'''\n",
    "\n",
    "\n",
    "X_train = (X_train[:,:,:,:]/255).astype('float32')\n",
    "Y_image = (Y_image[:,:,:,:]/255).astype('float32')\n",
    "\n",
    "#X_celeb = X_celeb.astype('float16')\n",
    "#X_celeb = np.resize(X_celeb, (train_size, 160, 160, 3))\n",
    "\n",
    "#X_test = testx/255\n",
    "#X_test = X_test.astype('float16')\n",
    "#X_test = np.resize(X_test, (test_size, 64, 64, 3))\n",
    "\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y\n",
    "  \n",
    "Y_train = convert_to_one_hot(Y_train[:], num_classes).T  \n",
    "#Y_test = convert_to_one_hot(testy, 8).T\n",
    "\n",
    "print(X_train.shape, Y_image.shape, Y_train.shape)\n",
    "#print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YzHUESwfeRn_"
   },
   "source": [
    "## Define Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ag4LN0YrlzWr"
   },
   "outputs": [],
   "source": [
    "def srcnn(X):\n",
    "\n",
    "    gen1 = tf.layers.conv2d(X, 64, 9, 1, padding = 'same')\n",
    "    P1 = tf.nn.relu(gen1)    \n",
    "    \n",
    "    gen2 = tf.layers.conv2d(P1, 32, 1, 1, padding = 'SAME')\n",
    "    P2 = tf.nn.relu(gen2)    \n",
    "\n",
    "    res_in_1 = P2\n",
    "    rc1 = tf.layers.conv2d(res_in_1, 32, 3, 1, padding = 'SAME')\n",
    "    rc1 = tf.nn.relu(rc1)\n",
    "    r1 = tf.layers.conv2d(rc1, 32, 1, 1, padding = 'SAME')\n",
    "    r1 = tf.nn.relu(r1)\n",
    "    r1 = tf.layers.conv2d(r1, 32, 1, 1, padding = 'SAME')\n",
    "    r1 = tf.nn.relu(r1)\n",
    "    res_out_1 = res_in_1 + r1    \n",
    "\n",
    "    res_in_2 = res_out_1\n",
    "    rc2 = tf.layers.conv2d(res_in_2, 32, 3, 1, padding = 'SAME')\n",
    "    rc2 = tf.nn.relu(rc2)\n",
    "    r2 = tf.layers.conv2d(rc2, 32, 1, 1, padding = 'SAME')\n",
    "    r2 = tf.nn.relu(r2)\n",
    "    r2 = tf.layers.conv2d(r2, 32, 1, 1, padding = 'SAME')\n",
    "    r2 = tf.nn.relu(r2)\n",
    "    res_out_2 = res_in_2 + r2 \n",
    "\n",
    "    res_in_3 = res_out_2\n",
    "    rc3 = tf.layers.conv2d(res_in_3, 32, 3, 1, padding = 'SAME')\n",
    "    rc3 = tf.nn.relu(rc3)\n",
    "    r3 = tf.layers.conv2d(rc3, 32, 1, 1, padding = 'SAME')\n",
    "    r3 = tf.nn.relu(r3)\n",
    "    r3 = tf.layers.conv2d(r3, 32, 1, 1, padding = 'SAME')\n",
    "    r3 = tf.nn.relu(r3)\n",
    "    res_out_3 = res_in_3 + r3\n",
    "\n",
    "    res_in_4 = res_out_3\n",
    "    rc4 = tf.layers.conv2d(res_in_4, 32, 3, 1, padding = 'SAME')\n",
    "    rc4 = tf.nn.relu(rc4)\n",
    "    r4 = tf.layers.conv2d(rc4, 32, 1, 1, padding = 'SAME')\n",
    "    r4 = tf.nn.relu(r4)\n",
    "    r4 = tf.layers.conv2d(r4, 32, 1, 1, padding = 'SAME')\n",
    "    r4 = tf.nn.relu(r4)\n",
    "    res_out_4 = res_in_4 + r4    \n",
    "\n",
    "    res_in_5 = res_out_4\n",
    "    rc5 = tf.layers.conv2d(res_in_5, 32, 3, 1, padding = 'SAME')\n",
    "    rc5 = tf.nn.relu(rc5)\n",
    "    r5 = tf.layers.conv2d(rc5, 32, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    r5 = tf.layers.conv2d(r5, 32, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    res_out_5 = res_in_5 + r5    \n",
    "\n",
    "    res_in_5 = res_out_5\n",
    "    rc5 = tf.layers.conv2d(res_in_5, 32, 3, 1, padding = 'SAME')\n",
    "    rc5 = tf.nn.relu(rc5)\n",
    "    r5 = tf.layers.conv2d(rc5, 32, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    r5 = tf.layers.conv2d(r5, 32, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    res_out_5 = res_in_5 + r5 \n",
    "\n",
    "    regen1 = tf.layers.conv2d(P2, 64, 5, 1, padding = 'same')\n",
    "\n",
    "    #print('res_out shape:', res_out_2.shape)\n",
    "    upsam = Upsample2xBlock(P2, kernel_size=3, filters=64)#tf.layers.conv2d_transpose(res_out_3,64,(3, 3),strides=(4,4), padding='same')\n",
    "    #[batch, height, width, in_channels]\n",
    "    #[height, width, output_channels, in_channels]\n",
    "    print('upsam shape:', upsam.shape)\n",
    "\n",
    "    res_in_5 = upsam\n",
    "    rc5 = tf.layers.conv2d(res_in_5, 32, 3, 1, padding = 'SAME')\n",
    "    rc5 = tf.nn.relu(rc5)\n",
    "    r5 = tf.layers.conv2d(rc5, 32, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    r5 = tf.layers.conv2d(r5, 4, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    res_out_5 = res_in_5 + r5 \n",
    "\n",
    "    res_in_5 = res_out_5\n",
    "    rc5 = tf.layers.conv2d(res_in_5, 32, 3, 1, padding = 'SAME')\n",
    "    rc5 = tf.nn.relu(rc5)\n",
    "    r5 = tf.layers.conv2d(rc5, 32, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    r5 = tf.layers.conv2d(r5, 4, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    res_out_5 = res_in_5 + r5 \n",
    "\n",
    "    res_in_5 = res_out_5\n",
    "    rc5 = tf.layers.conv2d(res_in_5, 32, 3, 1, padding = 'SAME')\n",
    "    rc5 = tf.nn.relu(rc5)\n",
    "    r5 = tf.layers.conv2d(rc5, 32, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    r5 = tf.layers.conv2d(r5, 4, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    res_out_5 = res_in_5 + r5 \n",
    "\n",
    "    res_in_5 = res_out_5\n",
    "    rc5 = tf.layers.conv2d(res_in_5, 32, 3, 1, padding = 'SAME')\n",
    "    rc5 = tf.nn.relu(rc5)\n",
    "    r5 = tf.layers.conv2d(rc5, 32, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    r5 = tf.layers.conv2d(r5, 4, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    res_out_5 = res_in_5 + r5 \n",
    "\n",
    "    res_in_5 = res_out_5\n",
    "    rc5 = tf.layers.conv2d(res_in_5, 32, 3, 1, padding = 'SAME')\n",
    "    rc5 = tf.nn.relu(rc5)\n",
    "    r5 = tf.layers.conv2d(rc5, 32, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    r5 = tf.layers.conv2d(r5, 4, 1, 1, padding = 'SAME')\n",
    "    r5 = tf.nn.relu(r5)\n",
    "    res_out_5 = res_in_5 + r5 \n",
    "\n",
    "    regen1 = tf.layers.conv2d(upsam, 3, 3, 1, padding = 'same')\n",
    "    A3 = tf.nn.relu(regen1)     \n",
    "    regen2 = tf.layers.conv2d(A3, 3, 3, 1, padding = 'same')\n",
    "    \n",
    "    print('output shape: ', regen2.shape)\n",
    "    \n",
    "    return regen1\n",
    "\n",
    "def inception(X):\n",
    "\n",
    "    A6, _ = inception_resnet_v2(X)\n",
    "\n",
    "    P_fl = tf.contrib.layers.flatten(A6)\n",
    "    fc = tf.contrib.layers.fully_connected(P_fl, num_classes, activation_fn = None)\n",
    "    \n",
    "    return fc\n",
    "\n",
    "\n",
    "def inference(input_images):\n",
    "    with slim.arg_scope([slim.conv2d], \n",
    "                         activation_fn=tf.nn.relu, stride=1, padding='SAME',\n",
    "                         weights_initializer=tf.truncated_normal_initializer(stddev=0.01)):\n",
    "                         # weights_initializer=tf.contrib.layers.xavier_initializer()):\n",
    "        x = slim.conv2d(input_images, 32, [3, 3],\n",
    "                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        padding='VALID', scope='conv1a')\n",
    "\n",
    "        x = slim.conv2d(x, 64, [3, 3], \n",
    "                        weights_initializer=tf.contrib.layers.xavier_initializer(), \n",
    "                        padding='VALID', scope='conv1b')\n",
    "\n",
    "        pool1b = slim.max_pool2d(x, [2, 2], stride=2, padding='VALID', scope='pool1b')\n",
    "\n",
    "        conv2_1 = slim.conv2d(pool1b, 64, [3, 3], scope='conv2_1')\n",
    "        conv2_2 = slim.conv2d(conv2_1, 64, [3, 3], scope='conv2_2')\n",
    "        res2_2 = pool1b + conv2_2\n",
    "        conv2 = slim.conv2d(res2_2, 128, [3, 3],\n",
    "                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        padding='VALID', scope='conv2')\n",
    "\n",
    "        pool2 = slim.max_pool2d(conv2, [2, 2], stride=2, padding='VALID', scope='pool2')\n",
    "        conv3_1 = slim.conv2d(pool2, 128, [3, 3], scope='conv3_1')\n",
    "        conv3_2 = slim.conv2d(conv3_1, 128, [3, 3], scope='conv3_2')\n",
    "        res3_2 = pool2 + conv3_2\n",
    "\n",
    "        conv3_3 = slim.conv2d(res3_2, 128, [3, 3], scope='conv3_3')\n",
    "        conv3_4 = slim.conv2d(conv3_3, 128, [3, 3], scope='conv3_4')\n",
    "        res3_4 = res3_2 + conv3_4\n",
    "\n",
    "        conv3 = slim.conv2d(res3_4, 256, [3, 3],\n",
    "                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        padding='VALID', scope='conv3')\n",
    "        pool3 = slim.max_pool2d(conv3, [2, 2], stride=2, padding='VALID', scope='pool3')\n",
    "        conv4_1 = slim.conv2d(pool3, 256, [3, 3], scope='conv4_1')\n",
    "        conv4_2 = slim.conv2d(conv4_1, 256, [3, 3], scope='conv4_2')\n",
    "        res4_2 = pool3 + conv4_2\n",
    "\n",
    "        conv4_3 = slim.conv2d(res4_2, 256, [3, 3], scope='conv4_3')\n",
    "        conv4_4 = slim.conv2d(conv4_3, 256, [3, 3], scope='conv4_4')\n",
    "        res4_4 = res4_2 + conv4_4\n",
    "\n",
    "        conv4_5 = slim.conv2d(res4_4, 256, [3, 3], scope='conv4_5')\n",
    "        conv4_6 = slim.conv2d(conv4_5, 256, [3, 3], scope='conv4_6')\n",
    "        res4_6 = res4_4 + conv4_6\n",
    "\n",
    "        conv4_7 = slim.conv2d(res4_6, 256, [3, 3], scope='conv4_7')\n",
    "        conv4_8 = slim.conv2d(conv4_7, 256, [3, 3], scope='conv4_8')\n",
    "        res4_8 = res4_6 + conv4_8\n",
    "\n",
    "        conv4_9 = slim.conv2d(res4_8, 256, [3, 3], scope='conv4_9')\n",
    "        conv4_10 = slim.conv2d(conv4_9, 256, [3, 3], scope='conv4_10')\n",
    "        res4_10 = res4_8 + conv4_10\n",
    "\n",
    "        conv4 = slim.conv2d(res4_10, 512, [3, 3],\n",
    "                        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                        padding='VALID', scope='conv4')\n",
    "        pool4 = slim.max_pool2d(conv4, [2, 2], stride=2, padding='VALID', scope='pool4')\n",
    "        \n",
    "        conv5_1 = slim.conv2d(pool4, 512, [3, 3], scope='conv5_1')\n",
    "        conv5_2 = slim.conv2d(conv5_1, 512, [3, 3], scope='conv5_2')\n",
    "        res5_2 = pool4 + conv5_2\n",
    "\n",
    "        conv5_3 = slim.conv2d(res5_2, 512, [3, 3], scope='conv5_3')\n",
    "        conv5_4 = slim.conv2d(conv5_3, 512, [3, 3], scope='conv5_4')\n",
    "        res5_4 = res5_2 + conv5_4\n",
    "\n",
    "        conv5_5 = slim.conv2d(res5_4, 512, [3, 3], scope='conv5_5')\n",
    "        conv5_6 = slim.conv2d(conv5_5, 512, [3, 3], scope='conv5_6')\n",
    "        res5_6 = res5_4 + conv5_6\n",
    "        res5_6 = slim.flatten(res5_6, scope='flatten')\n",
    "        feature = slim.fully_connected(res5_6, num_outputs=512, activation_fn=None, \n",
    "                            weights_initializer=tf.contrib.layers.xavier_initializer(), scope='fc1')\n",
    "\n",
    "        x = slim.fully_connected(feature, num_outputs=num_classes, activation_fn=None, scope='fc2')\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g6Js-vXU0aXf"
   },
   "outputs": [],
   "source": [
    "# Inception-Renset-A\n",
    "def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
    "    \"\"\"Builds the 35x35 resnet block.\"\"\"\n",
    "    with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):\n",
    "        with tf.variable_scope('Branch_0'):\n",
    "            tower_conv = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')\n",
    "        with tf.variable_scope('Branch_1'):\n",
    "            tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')\n",
    "        with tf.variable_scope('Branch_2'):\n",
    "            tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope='Conv2d_0b_3x3')\n",
    "            tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope='Conv2d_0c_3x3')\n",
    "        mixed = tf.concat([tower_conv, tower_conv1_1, tower_conv2_2], 3)\n",
    "        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
    "                         activation_fn=None, scope='Conv2d_1x1')\n",
    "        net += scale * up\n",
    "        if activation_fn:\n",
    "            net = activation_fn(net)\n",
    "    return net\n",
    "\n",
    "# Inception-Renset-B\n",
    "def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
    "    \"\"\"Builds the 17x17 resnet block.\"\"\"\n",
    "    with tf.variable_scope(scope, 'Block17', [net], reuse=reuse):\n",
    "        with tf.variable_scope('Branch_0'):\n",
    "            tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n",
    "        with tf.variable_scope('Branch_1'):\n",
    "            tower_conv1_0 = slim.conv2d(net, 128, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],\n",
    "                                        scope='Conv2d_0b_1x7')\n",
    "            tower_conv1_2 = slim.conv2d(tower_conv1_1, 192, [7, 1],\n",
    "                                        scope='Conv2d_0c_7x1')\n",
    "        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n",
    "        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
    "                         activation_fn=None, scope='Conv2d_1x1')\n",
    "        net += scale * up\n",
    "        if activation_fn:\n",
    "            net = activation_fn(net)\n",
    "    return net\n",
    "\n",
    "\n",
    "# Inception-Resnet-C\n",
    "def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):\n",
    "    \"\"\"Builds the 8x8 resnet block.\"\"\"\n",
    "    with tf.variable_scope(scope, 'Block8', [net], reuse=reuse):\n",
    "        with tf.variable_scope('Branch_0'):\n",
    "            tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')\n",
    "        with tf.variable_scope('Branch_1'):\n",
    "            tower_conv1_0 = slim.conv2d(net, 192, 1, scope='Conv2d_0a_1x1')\n",
    "            tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],\n",
    "                                        scope='Conv2d_0b_1x3')\n",
    "            tower_conv1_2 = slim.conv2d(tower_conv1_1, 256, [3, 1],\n",
    "                                        scope='Conv2d_0c_3x1')\n",
    "        mixed = tf.concat([tower_conv, tower_conv1_2], 3)\n",
    "        up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,\n",
    "                         activation_fn=None, scope='Conv2d_1x1')\n",
    "        net += scale * up\n",
    "        if activation_fn:\n",
    "            net = activation_fn(net)\n",
    "    return net\n",
    "  \n",
    "def inference(images, keep_probability, phase_train=True, \n",
    "              bottleneck_layer_size=128, weight_decay=0.0, reuse=None):\n",
    "    batch_norm_params = {\n",
    "        # Decay for the moving averages.\n",
    "        'decay': 0.995,\n",
    "        # epsilon to prevent 0s in variance.\n",
    "        'epsilon': 0.001,\n",
    "        # force in-place updates of mean and variance estimates\n",
    "        'updates_collections': None,\n",
    "        # Moving averages ends up in the trainable variables collection\n",
    "        'variables_collections': [ tf.GraphKeys.TRAINABLE_VARIABLES ],\n",
    "}\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                        weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                        weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                        normalizer_fn=slim.batch_norm,\n",
    "                        normalizer_params=batch_norm_params):\n",
    "        return inception_resnet_v2(images, is_training=phase_train,\n",
    "              dropout_keep_prob=keep_probability, bottleneck_layer_size=bottleneck_layer_size, reuse=reuse)\n",
    "\n",
    "\n",
    "def inception_resnet_v2(inputs, is_training=True,\n",
    "                        dropout_keep_prob=0.8,\n",
    "                        bottleneck_layer_size=128,\n",
    "                        reuse=None,\n",
    "                        scope='InceptionResnetV2'):\n",
    "    \"\"\"Creates the Inception Resnet V2 model.\n",
    "    Args:\n",
    "      inputs: a 4-D tensor of size [batch_size, height, width, 3].\n",
    "      num_classes: number of predicted classes.\n",
    "      is_training: whether is training or not.\n",
    "      dropout_keep_prob: float, the fraction to keep before final layer.\n",
    "      reuse: whether or not the network and its variables should be reused. To be\n",
    "        able to reuse 'scope' must be given.\n",
    "      scope: Optional variable_scope.\n",
    "    Returns:\n",
    "      logits: the logits outputs of the model.\n",
    "      end_points: the set of end_points from the inception model.\n",
    "    \"\"\"\n",
    "    end_points = {}\n",
    "  \n",
    "    with tf.variable_scope(scope, 'InceptionResnetV2', [inputs], reuse=reuse):\n",
    "        with slim.arg_scope([slim.batch_norm, slim.dropout],\n",
    "                            is_training=is_training):\n",
    "            with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
    "                                stride=1, padding='SAME'):\n",
    "      \n",
    "                # 149 x 149 x 32\n",
    "                net = slim.conv2d(inputs, 32, 3, stride=2, padding='VALID',\n",
    "                                  scope='Conv2d_1a_3x3')\n",
    "                end_points['Conv2d_1a_3x3'] = net\n",
    "                # 147 x 147 x 32\n",
    "                net = slim.conv2d(net, 32, 3, padding='VALID',\n",
    "                                  scope='Conv2d_2a_3x3')\n",
    "                end_points['Conv2d_2a_3x3'] = net\n",
    "                # 147 x 147 x 64\n",
    "                net = slim.conv2d(net, 64, 3, scope='Conv2d_2b_3x3')\n",
    "                end_points['Conv2d_2b_3x3'] = net\n",
    "                # 73 x 73 x 64\n",
    "                net = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
    "                                      scope='MaxPool_3a_3x3')\n",
    "                end_points['MaxPool_3a_3x3'] = net\n",
    "                # 73 x 73 x 80\n",
    "                net = slim.conv2d(net, 80, 1, padding='VALID',\n",
    "                                  scope='Conv2d_3b_1x1')\n",
    "                end_points['Conv2d_3b_1x1'] = net\n",
    "                # 71 x 71 x 192\n",
    "                net = slim.conv2d(net, 192, 3, padding='VALID',\n",
    "                                  scope='Conv2d_4a_3x3')\n",
    "                end_points['Conv2d_4a_3x3'] = net\n",
    "                # 35 x 35 x 192\n",
    "                net = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
    "                                      scope='MaxPool_5a_3x3')\n",
    "                end_points['MaxPool_5a_3x3'] = net\n",
    "        \n",
    "                # 35 x 35 x 320\n",
    "                with tf.variable_scope('Mixed_5b'):\n",
    "                    with tf.variable_scope('Branch_0'):\n",
    "                        tower_conv = slim.conv2d(net, 96, 1, scope='Conv2d_1x1')\n",
    "                    with tf.variable_scope('Branch_1'):\n",
    "                        tower_conv1_0 = slim.conv2d(net, 48, 1, scope='Conv2d_0a_1x1')\n",
    "                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,\n",
    "                                                    scope='Conv2d_0b_5x5')\n",
    "                    with tf.variable_scope('Branch_2'):\n",
    "                        tower_conv2_0 = slim.conv2d(net, 64, 1, scope='Conv2d_0a_1x1')\n",
    "                        tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,\n",
    "                                                    scope='Conv2d_0b_3x3')\n",
    "                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,\n",
    "                                                    scope='Conv2d_0c_3x3')\n",
    "                    with tf.variable_scope('Branch_3'):\n",
    "                        tower_pool = slim.avg_pool2d(net, 3, stride=1, padding='SAME',\n",
    "                                                     scope='AvgPool_0a_3x3')\n",
    "                        tower_pool_1 = slim.conv2d(tower_pool, 64, 1,\n",
    "                                                   scope='Conv2d_0b_1x1')\n",
    "                    net = tf.concat([tower_conv, tower_conv1_1,\n",
    "                                        tower_conv2_2, tower_pool_1], 3)\n",
    "        \n",
    "                end_points['Mixed_5b'] = net\n",
    "                net = slim.repeat(net, 10, block35, scale=0.17)\n",
    "        \n",
    "                # 17 x 17 x 1024\n",
    "                with tf.variable_scope('Mixed_6a'):\n",
    "                    with tf.variable_scope('Branch_0'):\n",
    "                        tower_conv = slim.conv2d(net, 384, 3, stride=2, padding='VALID',\n",
    "                                                 scope='Conv2d_1a_3x3')\n",
    "                    with tf.variable_scope('Branch_1'):\n",
    "                        tower_conv1_0 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
    "                        tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,\n",
    "                                                    scope='Conv2d_0b_3x3')\n",
    "                        tower_conv1_2 = slim.conv2d(tower_conv1_1, 384, 3,\n",
    "                                                    stride=2, padding='VALID',\n",
    "                                                    scope='Conv2d_1a_3x3')\n",
    "                    with tf.variable_scope('Branch_2'):\n",
    "                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
    "                                                     scope='MaxPool_1a_3x3')\n",
    "                    net = tf.concat([tower_conv, tower_conv1_2, tower_pool], 3)\n",
    "        \n",
    "                end_points['Mixed_6a'] = net\n",
    "                net = slim.repeat(net, 20, block17, scale=0.10)\n",
    "        \n",
    "                with tf.variable_scope('Mixed_7a'):\n",
    "                    with tf.variable_scope('Branch_0'):\n",
    "                        tower_conv = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
    "                        tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,\n",
    "                                                   padding='VALID', scope='Conv2d_1a_3x3')\n",
    "                    with tf.variable_scope('Branch_1'):\n",
    "                        tower_conv1 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
    "                        tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,\n",
    "                                                    padding='VALID', scope='Conv2d_1a_3x3')\n",
    "                    with tf.variable_scope('Branch_2'):\n",
    "                        tower_conv2 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')\n",
    "                        tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,\n",
    "                                                    scope='Conv2d_0b_3x3')\n",
    "                        tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,\n",
    "                                                    padding='VALID', scope='Conv2d_1a_3x3')\n",
    "                    with tf.variable_scope('Branch_3'):\n",
    "                        tower_pool = slim.max_pool2d(net, 3, stride=2, padding='VALID',\n",
    "                                                     scope='MaxPool_1a_3x3')\n",
    "                    net = tf.concat([tower_conv_1, tower_conv1_1,\n",
    "                                        tower_conv2_2, tower_pool], 3)\n",
    "        \n",
    "                end_points['Mixed_7a'] = net\n",
    "        \n",
    "                net = slim.repeat(net, 9, block8, scale=0.20)\n",
    "                net = block8(net, activation_fn=None)\n",
    "        \n",
    "                net = slim.conv2d(net, 1536, 1, scope='Conv2d_7b_1x1')\n",
    "                end_points['Conv2d_7b_1x1'] = net\n",
    "        \n",
    "                with tf.variable_scope('Logits'):\n",
    "                    end_points['PrePool'] = net\n",
    "                    #pylint: disable=no-member\n",
    "                    net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',\n",
    "                                          scope='AvgPool_1a_8x8')\n",
    "                    net = slim.flatten(net)\n",
    "          \n",
    "                    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\n",
    "                                       scope='Dropout')\n",
    "          \n",
    "                    end_points['PreLogitsFlatten'] = net\n",
    "                \n",
    "                net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None, \n",
    "                        scope='Bottleneck', reuse=False)\n",
    "  \n",
    "    return net, end_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2ayCpaqt-Gq"
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y_img, Y, mini_batch_size = 64, seed = 10):\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y_img = Y_img[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "        mini_batch_Y_img = shuffled_Y_img[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y_img, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
    "        mini_batch_Y_img = shuffled_Y_img[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y_img, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def PSNR(gt, pred):\n",
    "    gt = gt.astype(np.float64)\n",
    "    pred = pred.astype(np.float64)\n",
    "    mse = np.mean((pred - gt)**2)\n",
    "    psnr = 10*np.log10(255*255/mse)\n",
    "    return psnr\n",
    "    #return compare_psnr(gt, pred, data_range=255)\n",
    "    \n",
    "def SSIM(gt, pred):\n",
    "    ssim = compare_ssim(gt, pred, data_range=255, gaussian_weights=True)\n",
    "    return ssim\n",
    "\n",
    "def Upsample2xBlock(x, kernel_size, filters, strides=1):\n",
    "    x = tf.layers.conv2d(x, kernel_size=kernel_size, filters=filters, strides=strides, padding='same')\n",
    "    x = tf.depth_to_space(x, 8)\n",
    "    x = tf.nn.relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xn8f9VuqFql0"
   },
   "source": [
    "## Training Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Jo2ymlmxDmqH",
    "outputId": "3b71039b-c2b3-45ca-f39b-c8136d568cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (20000, 32, 32, 3)\n",
      "Y_train shape (20000, 1)\n",
      "Learning rate: 0.005\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-defbee1ebe0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_H0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_W0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_C0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mY_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_H1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_W1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_C1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Y_img'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "minibatch_size = 128\n",
    "learning_rate = 0.005\n",
    "num_epochs = 100001\n",
    "seed = 10\n",
    "print('X_train shape', X_train.shape)  \n",
    "print('Y_train shape', Y_train.shape)  \n",
    "print('Learning rate:', learning_rate)\n",
    "\n",
    "(m, n_H0, n_W0, n_C0) = X_train.shape  \n",
    "(m1, n_H1, n_W1, n_C1) = Y_image.shape             \n",
    "n_y = Y_train.shape[1]                            \n",
    "costs = []                                                            \n",
    "t1 = 0\n",
    "t2 = 0\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_H0, n_W0, n_C0], name = 'X')\n",
    "Y_img = tf.placeholder(tf.float32, [None, n_H1, n_W1, n_C1], name = 'Y_img')\n",
    "Y = tf.placeholder(tf.float32, [None, n_y], name = 'Y')\n",
    "\n",
    "Z3 = srcnn(X)\n",
    "#Z4 = squeeze_net(input=Z3, classes=num_classes)\n",
    "#Z4 = inception(Z3)\n",
    "#Z4 = inference(Z3)\n",
    "\n",
    "cost1 = tf.losses.mean_squared_error(Z3, Y_img)\n",
    "cost1 = tf.reduce_mean(cost1)\n",
    "#cost2 = tf.nn.softmax_cross_entropy_with_logits(logits = Z4, labels = Y)\n",
    "#cost2 = tf.reduce_mean(cost2)\n",
    "cost = cost1 #+ cost2\n",
    "\n",
    "#psnr = cost1 #PSNR(Y_img, Z3)\n",
    "#ssim = cost2 #SSIM(Y_img, Z3)\n",
    "#psnr = 10*np.log10(255*255/cost1)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)    \n",
    "init = tf.global_variables_initializer()    \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    print('Trainable Params:', np.sum([np.prod(v.shape) for v in tf.trainable_variables()]))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        minibatch_cost = 0.\n",
    "        num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X_train, Y_image, Y_train, minibatch_size, seed)\n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            (minibatch_X, minibatch_Yi, minibatch_Y) = minibatch\n",
    "            _ , temp_cost, temp_cost1 = sess.run([optimizer, cost, cost1], feed_dict = {X: minibatch_X, Y_img: minibatch_Yi})\n",
    "\n",
    "            #temp_cost += temp_cost / num_minibatches\n",
    "            temp_cost1 += temp_cost1 / num_minibatches\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            psnr = 10*np.log10(np.max(minibatch_Yi)*np.max(minibatch_Yi)/temp_cost1)\n",
    "            costs.append(temp_cost)\n",
    "            t2 = time.time()\n",
    "            print (\"Epoch:\", epoch, 'Time:', round(t2-t1, 1), 'Total loss:', round(temp_cost, 6), 'SR loss:', round(temp_cost1, 6), 'PSNR:', round(psnr, 6))\n",
    "            t1 = time.time()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print('Saving Model...')\n",
    "            saver.save(sess, '/content/drive/My Drive/Drive2(Panasonic)/Dataset/CelebA/model/model.ckpt')\n",
    "            print('Model Saved...')\n",
    "            Z31 = Z3[0].eval(feed_dict = {X: minibatch_X, Y_img: minibatch_Yi})\n",
    "            plt.imshow(Z31)\n",
    "            plt.show()\n",
    "            #!nvidia-smi\n",
    "\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    predict_op = tf.argmax(Z4, 1)\n",
    "    correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('accuracy', accuracy)\n",
    "    train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "    #test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    #print(\"Test Accuracy:\", test_accuracy)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unQHAgkQx0mR"
   },
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "imported_meta = tf.train.import_meta_graph(\"/content/drive/My Drive/Drive2(Panasonic)/Dataset/CelebA/model/model.ckpt.meta\")\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "minibatch_size = 64\n",
    "seed = 10\n",
    "                           \n",
    "\n",
    "costs = []                                                            \n",
    "t1 = 0\n",
    "t2 = 0\n",
    "\n",
    "(m, n_H0, n_W0, n_C0) = X_train.shape  \n",
    "#(m1, n_H1, n_W1, n_C1) = Y_image.shape             \n",
    "n_y = Y_train.shape[1]                            \n",
    "costs = []                                                            \n",
    "t1 = 0\n",
    "t2 = 0\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_H0, n_W0, n_C0], name = 'X')\n",
    "#Y_img = tf.placeholder(tf.float32, [None, n_H1, n_W1, n_C1], name = 'Y_img')\n",
    "Y = tf.placeholder(tf.float32, [None, n_y], name = 'Y')\n",
    "\n",
    "Z3 = srcnn(X)\n",
    "#Z4 = inference(Z3)\n",
    "Z4 = inception(Z3)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    imported_meta.restore(sess, '/content/drive/My Drive/Drive2(Panasonic)/Dataset/CelebA/model/model.ckpt')\n",
    "    print(\"restored\")\n",
    "    #print(np.sum([np.prod(v.shape) for v in tf.trainable_variables()]))\n",
    "    correct_prediction = tf.equal(tf.argmax(Z4, 1), tf.argmax(Y, 1))\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    train_accuracy = correct_prediction.eval({X: X_train[0:1000,:,:,:], Y: Y_train[0:1000]})\n",
    "    #, X_t: X_tiny, Y_t:Y_tiny\n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "    #5yy73k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EYvZXYfA-L_o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "Gmcg_3v5_yUy",
    "outputId": "0b966d20-4131-49bf-894d-40ec9d842bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9zYjSfMo_R1g"
   },
   "outputs": [],
   "source": [
    "initial = [1,2,3,4]\n",
    "initial.append(2)\n",
    "file=open('/content/cost.txt','w')\n",
    "file.write(str(cost))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PxanlLfA1m1V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "my_model_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
